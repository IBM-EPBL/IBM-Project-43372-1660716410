{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"b25c8ef1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"IMPORTING LIBARIES\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 32,\n",
    "   \"id\": \"6cb61274\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from tensorflow import keras\\n\",\n",
    "    \"from tensorflow.keras.models import Sequential\\n\",\n",
    "    \"from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\\n\",\n",
    "    \"from tensorflow.keras.optimizers import Adam\\n\",\n",
    "    \"from tensorflow.keras.metrics import categorical_crossentropy\\n\",\n",
    "    \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "    \"from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"170ff2ba\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"DATA PREPROCESSING\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 33,\n",
    "   \"id\": \"c894fb3d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip= True)\\n\",\n",
    "    \"test_datagen=ImageDataGenerator(rescale=1./255)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 34,\n",
    "   \"id\": \"02a92a9f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Found 2626 images belonging to 5 classes.\\n\",\n",
    "      \"Found 1055 images belonging to 5 classes.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"x_train=train_datagen.flow_from_directory(r'C:\\\\Users\\\\91720\\\\Downloads\\\\Nutrition_Image_Analysis\\\\Dataset\\\\TRAIN_SET'\\n\",\n",
    "    \"                                         ,target_size=(64,64),batch_size=32,class_mode='categorical'\\n\",\n",
    "    \"                                         )\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_test=test_datagen.flow_from_directory(r'C:\\\\Users\\\\91720\\\\Downloads\\\\Nutrition_Image_Analysis\\\\Dataset\\\\TEST_SET'\\n\",\n",
    "    \"                                         ,target_size=(64,64),batch_size=32,class_mode='categorical')  \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 35,\n",
    "   \"id\": \"d0967849\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"{'APPLES': 0, 'BANANA': 1, 'ORANGE': 2, 'PINEAPPLE': 3, 'WATERMELON': 4}\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 35,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"x_train.class_indices\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c288105c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"MODEL BUILDING\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 36,\n",
    "   \"id\": \"dc0a1f09\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model=Sequential()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 37,\n",
    "   \"id\": \"56ecedb7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model=Sequential()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 38,\n",
    "   \"id\": \"cfcc053f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"classifer=Sequential()\\n\",\n",
    "    \"classifer.add(Conv2D(32,(3,3),input_shape=(64,64,3),activation=\\\"relu\\\"))\\n\",\n",
    "    \"classifer.add(MaxPool2D(pool_size=(2,2)))\\n\",\n",
    "    \"classifer.add(Conv2D(32,(3,3),activation=\\\"relu\\\"))\\n\",\n",
    "    \"classifer.add(MaxPool2D(pool_size=(2,2)))\\n\",\n",
    "    \"classifer.add(Flatten())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 39,\n",
    "   \"id\": \"d13820f0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"classifer.add(Dense(units=128,activation=\\\"relu\\\"))\\n\",\n",
    "    \"classifer.add(Dense(units=5,activation=\\\"softmax\\\"))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 40,\n",
    "   \"id\": \"a58918f1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Model: \\\"sequential_6\\\"\\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"Layer (type)                 Output Shape              Param #   \\n\",\n",
    "      \"=================================================================\\n\",\n",
    "      \"conv2d_5 (Conv2D)            (None, 62, 62, 32)        896       \\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"max_pooling2d_5 (MaxPooling2 (None, 31, 31, 32)        0         \\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"conv2d_6 (Conv2D)            (None, 29, 29, 32)        9248      \\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"flatten_3 (Flatten)          (None, 6272)              0         \\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"dense_6 (Dense)              (None, 128)               802944    \\n\",\n",
    "      \"_________________________________________________________________\\n\",\n",
    "      \"dense_7 (Dense)              (None, 5)                 645       \\n\",\n",
    "      \"=================================================================\\n\",\n",
    "      \"Total params: 813,733\\n\",\n",
    "      \"Trainable params: 813,733\\n\",\n",
    "      \"Non-trainable params: 0\\n\",\n",
    "      \"_________________________________________________________________\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"classifer.summary()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 41,\n",
    "   \"id\": \"2ac4cdfd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"classifer.compile(optimizer=\\\"rmsprop\\\",loss=\\\"categorical_crossentropy\\\",metrics=[\\\"accuracy\\\"])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 42,\n",
    "   \"id\": \"de0dcb34\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Epoch 1/20\\n\",\n",
    "      \"82/82 [==============================] - 7s 77ms/step - loss: 0.5474 - accuracy: 0.8150 - val_loss: 0.0931 - val_accuracy: 0.9632\\n\",\n",
    "      \"Epoch 2/20\\n\",\n",
    "      \"82/82 [==============================] - 6s 73ms/step - loss: 0.0608 - accuracy: 0.9869 - val_loss: 0.0498 - val_accuracy: 0.9688\\n\",\n",
    "      \"Epoch 3/20\\n\",\n",
    "      \"82/82 [==============================] - 9s 105ms/step - loss: 0.1224 - accuracy: 0.9815 - val_loss: 0.0258 - val_accuracy: 0.9810\\n\",\n",
    "      \"Epoch 4/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 103ms/step - loss: 0.1547 - accuracy: 0.9865 - val_loss: 8.6944e-04 - val_accuracy: 1.0000\\n\",\n",
    "      \"Epoch 5/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 98ms/step - loss: 6.5788e-04 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 0.9900\\n\",\n",
    "      \"Epoch 6/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 102ms/step - loss: 0.1110 - accuracy: 0.9907 - val_loss: 0.0077 - val_accuracy: 1.0000\\n\",\n",
    "      \"Epoch 7/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 99ms/step - loss: 0.0903 - accuracy: 0.9880 - val_loss: 0.5883 - val_accuracy: 0.8694\\n\",\n",
    "      \"Epoch 8/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 95ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0128 - val_accuracy: 0.9933\\n\",\n",
    "      \"Epoch 9/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 100ms/step - loss: 0.1629 - accuracy: 0.9877 - val_loss: 0.1352 - val_accuracy: 0.9710\\n\",\n",
    "      \"Epoch 10/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 97ms/step - loss: 7.3137e-05 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9989\\n\",\n",
    "      \"Epoch 11/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 96ms/step - loss: 0.0934 - accuracy: 0.9907 - val_loss: 5.1888e-04 - val_accuracy: 1.0000\\n\",\n",
    "      \"Epoch 12/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 101ms/step - loss: 1.2171e-05 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 0.9877\\n\",\n",
    "      \"Epoch 13/20\\n\",\n",
    "      \"82/82 [==============================] - 11s 135ms/step - loss: 0.0364 - accuracy: 0.9942 - val_loss: 0.0259 - val_accuracy: 0.9799\\n\",\n",
    "      \"Epoch 14/20\\n\",\n",
    "      \"82/82 [==============================] - 9s 108ms/step - loss: 1.1797e-06 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9821\\n\",\n",
    "      \"Epoch 15/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 98ms/step - loss: 0.0680 - accuracy: 0.9927 - val_loss: 0.0026 - val_accuracy: 0.9989\\n\",\n",
    "      \"Epoch 16/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 101ms/step - loss: 8.1301e-06 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 0.9933\\n\",\n",
    "      \"Epoch 17/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 101ms/step - loss: 0.0381 - accuracy: 0.9954 - val_loss: 0.7631 - val_accuracy: 0.8795\\n\",\n",
    "      \"Epoch 18/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 99ms/step - loss: 4.0489e-05 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9732\\n\",\n",
    "      \"Epoch 19/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 95ms/step - loss: 0.0150 - accuracy: 0.9973 - val_loss: 0.0305 - val_accuracy: 0.9855\\n\",\n",
    "      \"Epoch 20/20\\n\",\n",
    "      \"82/82 [==============================] - 8s 98ms/step - loss: 1.4313e-05 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 0.9888\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<tensorflow.python.keras.callbacks.History at 0x1ff99d56850>\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 42,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"classifer.fit(x_train,steps_per_epoch=82,epochs=20,validation_data=x_test,validation_steps=28)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a4d1c523\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"SAVING THE MODEL\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 43,\n",
    "   \"id\": \"049acc66\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"classifer.save(\\\"model.h5\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9f15e354\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"MODEL PREDICITION\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 44,\n",
    "   \"id\": \"7c7d378d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<tensorflow.python.keras.engine.sequential.Sequential at 0x1ff9a7cac40>\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 44,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from tensorflow.keras.models import load_model\\n\",\n",
    "    \"model=load_model(\\\"model.h5\\\")\\n\",\n",
    "    \"model\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 45,\n",
    "   \"id\": \"6c6a83c9\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from tensorflow.keras.preprocessing import image\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 46,\n",
    "   \"id\": \"31bb6c02\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"img = image.load_img(r\\\"C:\\\\Users\\\\91720\\\\Downloads\\\\Nutrition_Image_Analysis\\\\Dataset\\\\TEST_SET\\\\BANANA\\\\13_100.jpg\\\", target_size = (64, 64))\\n\",\n",
    "    \"x = image.img_to_array(img)\\n\",\n",
    "    \"x = np.expand_dims(x, axis = 0)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 47,\n",
    "   \"id\": \"84deac01\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"array([[0., 1., 0., 0., 0.]], dtype=float32)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 47,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"prediction = model.predict(x)\\n\",\n",
    "    \"index = ['APPLES', 'BANANA', 'ORANGE', 'PINEAPPLE', 'WATERMELON']\\n\",\n",
    "    \"prediction\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 48,\n",
    "   \"id\": \"50aae8f5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"ind = np.argmax(prediction)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 49,\n",
    "   \"id\": \"730895a1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"'BANANA'\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 49,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"index[ind]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"6321dcfd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.7\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
